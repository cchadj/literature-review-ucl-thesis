
@inproceedings{dahl_improving_2013,
	title = {Improving deep neural networks for {LVCSR} using rectified linear units and dropout},
	doi = {10.1109/ICASSP.2013.6639346},
	abstract = {Recently, pre-trained deep neural networks ({DNNs}) have outperformed traditional acoustic models based on Gaussian mixture models ({GMMs}) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random “dropout” procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid over-fitting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectified linear unit ({ReLU}) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modified deep neural networks using {ReLUs} trained with dropout during frame level training provide an 4.2\% relative improvement over a {DNN} trained with sigmoid units, and a 14.4\% relative improvement over a strong {GMM}/{HMM} system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code.},
	eventtitle = {2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
	pages = {8609--8613},
	booktitle = {2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
	author = {Dahl, George E. and Sainath, Tara N. and Hinton, Geoffrey E.},
	date = {2013-05},
	note = {{ISSN}: 2379-190X},
	keywords = {acoustic modeling, acoustic models, acoustic signal processing, Acoustics, Bayes methods, Bayesian optimization, Bayesian optimization code, broadcast news, computer vision tasks, deep learning, deep neural networks, {DNN}, dropout, English broadcast news task, frame level training, Gaussian mixture models, Gaussian processes, generalization error, {GMM}/{HMM} system, hidden Markov models, Hidden Markov models, hyper-parameter tuning, {LVCSR}, neural nets, neural networks, Neural networks, optimisation, Optimization, random dropout procedure, rectified linear unit, rectified linear units, {ReLU} nonlinearities, sigmoid units, small-scale phone recognition task, Speech recognition, Training, vocabulary speech recognition benchmarks}
}